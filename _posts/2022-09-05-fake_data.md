---
layout: post
output:
  md_document:
    preserve_yaml: true
    variant: markdown_github
subtitle: A Cautionary Demonstration
author: benjamin_chase
tags:
- causal inference
- data mining
readtime: True
last-updated: September 5, 2022
title: The Danger of Wide Nets & Shotgun Regressions
image: https://benchase33.github.io/testing.github.io/assets/conf_effmod_img/dag_3.png
---

In one of our <a href = 'https://unbiaseddatablog.github.io/2022-06-18-confounding_modification/'> previous posts</a>, we briefly discussed the importance of forming a precise research question when estimating causal effects. Precise research questions are important because they minimize the risk of false positives and otherwise misleading results. Consider the following two questions:

1. Does one hour of aerobic exercise every morning for two months lower individual's resting heart rates?
2. Which factors lower individual's resting heart rates?

The first question is fairly precise and leads to a clear causal inference analysis. We could divide subjects into two treatment groups, monitor their resting heart rate for several months, and conduct a *difference-in-differences* analysis. We might be concerned about pitfalls such as unobserved *compliance* issues, but that's a problem for another day. In general, this question and approach offer a reasonable and respectable start to estimating a causal effect.

The second question, however, does not suggest a clear approach. What would the treatment groups be? Do we need to conduct multiple experiments? How do we know which potential factors are even worth considering? A typical approach is to take a bunch of potential factors (e.g., daily caloric intake, caffeine consumption), a set of control variables (age, height, weight), and toss them all into a linear regression. The answer to the question, then, is the list of factors whose estimated coefficients are significantly different from zero (i.e., a p-value less than 0.05). 

The first approach is analogous to a sniper rifle. You pick out a precise target, measure the wind speed, calculate the trajectory of your bullet, and take a slow, steady shot. The second approach is like throwing a giant fishing net into the ocean. You have no idea what you're going to get and all sorts of junk might get dragged up from the bottom. When we estimate causal effects, we want to use sniper rifles. Wide nets have their places (e.g., identifying trends, motivating research questions) but are best left at home when we need a bullseye.

The rest of this post will cover problems with taking a spray and pray approach to estimating causal effects.

# Confounding Must be Handled Case-by-Case

Consider, in our attempt to find out which factors lower an individual's resting heart rate, we identify a list of 50 different variables: race, biological sex, age, weight, height, education, and so on. We find a de-identified database of medical data with all our desired variables and are off to the races. For each variable, we perform either a t-test (binary) or an F-test (categorical), and write down whether the test yielded a significant result (e.g., a p-value less than 0.05). When we're done, we have a reduced list of variables which we claim are the set of factors which can lower an individual's blood pressure. Sounds great, right? Not quite!

There are a littany of reasons this approach would **not** yield a defensible estimate of any causal effects.

# Statistical Tests Don't Have Thumbs

First, those t-tests and F-tests we ran had no way to handle confounding. If we found a significant effect for both biological sex and age, but then realized every female in our data was over the age of 65, it would be hard to reject that age was a confounder when measuring the effect of biological sex. Similarly, if we found a negative effect for height and a positive effect for weight, but every subject with a bachelor's degree was short and obese, we won't get a reliable estimate for the effect of education. 

# Statistical Tests Can't See

Statistical tests only have access to the data we provide. When we run a t-test, all the computer does is a series of math operations to determine the likelihood the two populations came from a distribution with the same mean. If we compare enough random things in the world with statistical tests, we're going to get lots of results that appear significant but aren't meaningful nor causal. If we only used t-tests and F-tests, we would think people cook dinner because the sun goes down, schools close because the weather becomes hot, and that getting taller makes you older.

# Statistical Tests Work With Finite Data

Sometimes patterns emerge despite a lack of real-world meaning nor cause-and-effect relationship. Sometimes the world is just random. Whenever we collect data, we capture this randomness; that's why *power analysis* and *sample size* are important aspects to get right when doing research. This randomness gives rise to patterns in data that don't represent any tangible process; sometimes the average height of males in Walmart happens to be lower than the average height of females in Walmart, just because that happens sometimes. It would be weird if it didn't, at least every so often. Statistical tests can't tell the difference between these phantom patterns and real patterns. Collecting more data can reduce the odds of meaningless patterns, but they are always there.

# Statistical Tests in the Spotlight

All t-tests and F-tests can do is quantify superficial relationships, and many times they can't even do that.


# The Danger of Shotgun Regressions

So, running a bunch of t-tests and F-tests is a bad idea. What if we take things up a notch and run a regression? Throwing all our variables in together with some additional control variables should iron things out, right? 



**Note**: All source code written for this post is available in a Python
notebook here: <https://colab.research.google.com/drive/1e7WMoYxcrU33WyZTQkIEQd1S9H44trvb?usp=sharing>

<a name="myfootnote1">1</a>: An excellent resource which covers more ways to use propensity scores and alternatives to logistic regression: <https://www.rand.org/pubs/presentations/PT147.html><a href="#footnote-1-ref">&#8617;</a>

<a name="myfootnote2">2</a>: Check out this book for a ground-up explanation of marginal exchangeability and all things related to causal inference: <https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/><a href="#footnote-2-ref">&#8617;</a>

<a name="myfootnote3">3</a>:. An additional resource for understanding and applying propensity score matching: <https://sites.google.com/site/econometricsacademy/econometrics-models/propensity-score-matching><a href="#footnote-3-ref">&#8617;</a>
